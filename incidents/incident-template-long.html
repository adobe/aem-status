<h1 class="minor">[Brief incident title - e.g., "Publishing delays due to API rate limiting"]</h1>
<article
    data-incident-start-time="[ISO 8601 timestamp - e.g., 2024-01-15T14:30:00.000Z]"
    data-incident-end-time="[ISO 8601 timestamp - e.g., 2024-01-15T16:45:00.000Z]"
    data-incident-error-rate="[Error rate as decimal - e.g., 0.05 for 5%]"
    data-incident-impacted-service="[delivery|publishing]">

    <h3>Executive Summary</h3>
    <p>
        [Provide a concise 2-4 sentence overview of the incident. Include: when it occurred,
        what service was affected, the primary impact to customers, and the root cause. This
        should be understandable to non-technical stakeholders.]
    </p>
    <p>
        Example: On January 15, 2024, between 2:30 PM and 4:45 PM UTC, customers experienced
        publishing delays averaging 5 minutes. The incident was caused by an unexpected surge
        in API requests triggering rate limits on our GitHub integration. Approximately 15%
        of publishing operations were affected. The issue was resolved by implementing request
        throttling and increasing rate limits.
    </p>

    <h3>Incident Timeline</h3>
    <p>
        [Provide a detailed chronological account of the incident. Use UTC timestamps. Include
        key events from detection through resolution:]
    </p>
    <ul>
        <li><strong>14:30 UTC</strong> - [First indication of the problem - e.g., monitoring alerts, customer reports]</li>
        <li><strong>14:35 UTC</strong> - [Initial response actions taken]</li>
        <li><strong>14:50 UTC</strong> - [Key discoveries or escalations]</li>
        <li><strong>15:15 UTC</strong> - [Attempted remediation steps]</li>
        <li><strong>16:30 UTC</strong> - [Successful resolution implemented]</li>
        <li><strong>16:45 UTC</strong> - [Service fully restored and verified]</li>
    </ul>

    <h3>Impact Analysis</h3>
    <p>
        [Quantify the customer impact. Include specific metrics:]
    </p>
    <ul>
        <li>Number or percentage of affected customers</li>
        <li>Duration of the incident</li>
        <li>Affected services or features</li>
        <li>Business impact (e.g., failed deployments, error rates, latency increases)</li>
        <li>Geographic scope if applicable</li>
    </ul>
    <p>
        [Example: The incident affected 12 customers (approximately 15% of active users during
        this timeframe) for 2 hours and 15 minutes. Publishing operations experienced an average
        delay of 5 minutes, with some operations timing out entirely. No data loss occurred, and
        all failed operations could be retried successfully after the incident was resolved.]
    </p>

    <h3>Root Cause Analysis</h3>
    <p>
        [Identify the fundamental reason why the incident occurred. Go beyond the immediate
        trigger to identify systematic causes. Consider using the "Five Whys" technique:]
    </p>
    <ul>
        <li>What was the underlying technical cause?</li>
        <li>Why did our systems fail to handle this condition?</li>
        <li>What process or design decisions contributed to the failure?</li>
        <li>Why didn't our monitoring catch this sooner?</li>
    </ul>
    <p>
        [Example: The root cause was a shared rate limit pool for GitHub API requests across
        all customers. Our architecture assumed relatively uniform request patterns, but lacked
        per-customer throttling to prevent one customer's usage from affecting others. Additionally,
        our monitoring only tracked aggregate API usage, not per-customer patterns, preventing
        early detection of abnormal behavior.]
    </p>

    <h3>Trigger</h3>
    <p>
        [Describe the specific event or condition that initiated the incident. This is distinct
        from the root cause - it's the immediate precipitating factor:]
    </p>
    <p>
        [Example: A customer's newly deployed automated testing framework began making rapid
        successive API calls to verify content publishing. The framework was configured to check
        publishing status every second for 100 different content items, generating 6,000 API
        calls in the first 10 minutes of operation.]
    </p>

    <h3>Resolution</h3>
    <p>
        [Describe how the incident was resolved. Include both immediate mitigation and any
        longer-term fixes:]
    </p>
    <p>
        <strong>Immediate Mitigation:</strong>
        [Example: At 15:15 UTC, we implemented emergency rate limiting on the customer's API
        token, reducing their request rate to sustainable levels. This immediately stopped the
        rate limit exhaustion.]
    </p>
    <p>
        <strong>Permanent Resolution:</strong>
        [Example: At 16:30 UTC, we deployed per-customer rate limiting across the platform to
        prevent any single customer from exhausting shared resources. We also increased the base
        rate limit allocation and implemented request queuing to handle burst traffic more gracefully.]
    </p>

    <h3>Detection</h3>
    <p>
        [Explain how the incident was discovered:]
    </p>
    <ul>
        <li>Was it caught by automated monitoring/alerting?</li>
        <li>Did customers report it first?</li>
        <li>How long between incident start and detection?</li>
        <li>Why did detection take this long (if applicable)?</li>
    </ul>
    <p>
        [Example: The incident was initially detected through customer support tickets reporting
        publishing delays at 14:35 UTC, approximately 5 minutes after the incident began. Our
        automated monitoring did not alert because aggregate API usage remained within normal
        thresholds - the issue was in the distribution of requests, not the total volume.]
    </p>

    <h3>What Went Well</h3>
    <p>
        [Identify positive aspects of the incident response. This helps reinforce good practices:]
    </p>
    <ul>
        <li>[Example: The incident response team was assembled within 10 minutes of detection]</li>
        <li>[Example: Customer communication was proactive and transparent throughout]</li>
        <li>[Example: Our rollback procedures worked as designed]</li>
        <li>[Example: Cross-team collaboration between engineering and customer success was excellent]</li>
    </ul>

    <h3>What Could Have Gone Better</h3>
    <p>
        [Honestly assess areas for improvement. Focus on systems and processes, not individuals:]
    </p>
    <ul>
        <li>[Example: Detection relied on customer reports rather than automated monitoring]</li>
        <li>[Example: Initial diagnosis took 45 minutes due to unclear logging around rate limiting]</li>
        <li>[Example: We lacked runbook documentation for API rate limit incidents]</li>
        <li>[Example: Communication between the incident response team and customer success could have been more structured]</li>
    </ul>

    <h3>Lessons Learned</h3>
    <p>
        [Synthesize key insights from this incident that have broader applicability:]
    </p>
    <ul>
        <li>[Example: Shared resource pools without per-tenant isolation create systemic risk in multi-tenant systems]</li>
        <li>[Example: Aggregate monitoring can mask per-customer anomalies that affect service quality]</li>
        <li>[Example: Automated customer testing frameworks can generate traffic patterns significantly different from human usage]</li>
        <li>[Example: Clear escalation paths and communication protocols are essential for rapid incident response]</li>
    </ul>

    <h3>Action Items</h3>
    <p>
        We have identified the following action items to prevent similar incidents and improve our response capabilities:
    </p>

    <h4>Monitoring and Alerting Improvements</h4>
    <ol>
        <li>[Action item with owner and deadline - e.g., "Implement per-customer API usage monitoring and alerting (Owner: Alice, Due: Jan 30)"]</li>
        <li>[Action item - e.g., "Add rate limiting alerts that trigger before limits are reached (Owner: Bob, Due: Jan 25)"]</li>
        <li>[Action item - e.g., "Create dashboard showing API usage distribution across customers (Owner: Carol, Due: Feb 5)"]</li>
    </ol>

    <h4>Technical Improvements</h4>
    <ol>
        <li>[Action item - e.g., "Implement per-customer rate limit pools to prevent noisy neighbor issues (Owner: Dave, Due: Feb 15)"]</li>
        <li>[Action item - e.g., "Add request queuing with exponential backoff for rate limit scenarios (Owner: Eve, Due: Feb 20)"]</li>
        <li>[Action item - e.g., "Review and increase base rate limits based on usage analysis (Owner: Frank, Due: Jan 28)"]</li>
    </ol>

    <h4>Documentation and Process</h4>
    <ol>
        <li>[Action item - e.g., "Create runbook for API rate limiting incidents (Owner: Grace, Due: Jan 22)"]</li>
        <li>[Action item - e.g., "Document best practices for customer API integration (Owner: Henry, Due: Feb 1)"]</li>
        <li>[Action item - e.g., "Update incident response procedures with API-specific escalation paths (Owner: Iris, Due: Jan 26)"]</li>
    </ol>

    <h4>Customer Communication</h4>
    <ol>
        <li>[Action item - e.g., "Develop automated notifications for customers approaching rate limits (Owner: Jack, Due: Feb 10)"]</li>
        <li>[Action item - e.g., "Create customer-facing documentation about rate limits and best practices (Owner: Kate, Due: Jan 29)"]</li>
    </ol>

    <time>[ISO 8601 timestamp when postmortem was completed - e.g., 2024-01-16T10:00:00.000Z]</time>
</article>

<ul class="updates">
    <li>
        <h2>Resolved</h2>
        <p>[Description of final resolution - e.g., "This incident has been resolved. All services are operating normally."]</p>
        <time>[ISO 8601 timestamp - e.g., 2024-01-15T16:45:00.000Z]</time>
    </li>

    <li>
        <h2>Monitoring</h2>
        <p>[Description of monitoring phase - e.g., "We have implemented a fix and are monitoring the situation. Publishing operations are returning to normal."]</p>
        <time>[ISO 8601 timestamp - e.g., 2024-01-15T16:30:00.000Z]</time>
    </li>

    <li>
        <h2>Identified</h2>
        <p>[Description of when issue was identified and initial response - e.g., "We have identified the root cause as API rate limiting. Our team is implementing per-customer throttling to resolve the issue."]</p>
        <time>[ISO 8601 timestamp - e.g., 2024-01-15T15:15:00.000Z]</time>
    </li>

    <li>
        <h2>Investigating</h2>
        <p>[Initial incident report - e.g., "We are investigating reports of publishing delays affecting some customers. Our team is actively working to identify the cause."]</p>
        <time>[ISO 8601 timestamp - e.g., 2024-01-15T14:35:00.000Z]</time>
    </li>
</ul>
